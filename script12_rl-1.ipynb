{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<h1>강화학습</h1>"],"metadata":{"id":"xo8WJWYS3jPa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6fTCJpy3aHY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756037807090,"user_tz":-540,"elapsed":9289,"user":{"displayName":"colab c","userId":"17273071048194274415"}},"outputId":"8ef6cd5f-4ff2-40da-ecd6-c241f53c611a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym==0.23 in /usr/local/lib/python3.12/dist-packages (0.23.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from gym==0.23) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym==0.23) (3.1.1)\n","Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym==0.23) (0.1.0)\n"]}],"source":["# gym 라이브러리 설치\n","# 강화학습 연구와 교육을 위해 각종 환경과 API를 제공\n","!pip install gym==0.23"]},{"cell_type":"code","source":["# 라이브러리\n","import gym\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# 경고 메시지 출력하지 않기\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"0VNyrAFV4jGw","executionInfo":{"status":"ok","timestamp":1756037811936,"user_tz":-540,"elapsed":339,"user":{"displayName":"colab c","userId":"17273071048194274415"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8f4dcacf-a07e-460a-a976-8dd37621a26a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n","Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n","See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"]}]},{"cell_type":"markdown","source":["# Fronzen Lake + Q-Learning"],"metadata":{"id":"VzbeDpgk456t"}},{"cell_type":"markdown","source":["환경 설정"],"metadata":{"id":"CAx0F6zL8q8o"}},{"cell_type":"code","source":["# 강의에서 다룬 Fronzen Lake 예제\n","# 상태: 구멍(H)-출발(S)-얼음(F)-얼음(F)-목적지(G) 의 구조\n","# 행동: 0(LEFT), 1(DOWN), 2(RIGHT), 3(UP)\n","#       여기서는 2개만 허용: 0(LEFT), 2(RIGHT)\n","# 보상: G은 +1, 나머지는 0\n","env = gym.make('FrozenLake-v1',desc=['HSFFG'],is_slippery=False)"],"metadata":{"id":"gTNJu1k945nb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["가치함수와 정책함수"],"metadata":{"id":"mW25lsKQ8smA"}},{"cell_type":"code","source":["# 가치함수: Q테이블, 0으로 초기화\n","Q = np.zeros([env.observation_space.n, env.action_space.n])\n","Q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPk6ET-t5Ibr","outputId":"bad6f92e-291c-445d-c48c-c9aec229364b","executionInfo":{"status":"ok","timestamp":1756037815217,"user_tz":-540,"elapsed":5,"user":{"displayName":"colab c","userId":"17273071048194274415"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# 정책 함수, e는 e-greedy 변수\n","# 4의 행동에 대한 Q값(L,D,R,U)을 받아 행동을 결정\n","# 이 중에서 L/R만 사용\n","def policy(qs,e=0):\n","  if e > random.random():\n","    return random.choice([0,2])\n","  else:\n","    if qs[0] > qs[2]: return 0\n","    elif qs[0] < qs[2]: return 2\n","    else: return random.choice([0,2])"],"metadata":{"id":"20IMRyJS7hyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델 훈련"],"metadata":{"id":"fxukz6vG80xz"}},{"cell_type":"code","source":["num_episodes = 100   # 필요하다면 더 늘려서 수행\n","discount = 0.9\n","Q = Q*0.0  # Q함수 초기화\n","for i in range(num_episodes):\n","  state = env.reset()   # 초기 상태로 리셋\n","  done = False\n","  while not done:\n","    action = policy(Q[state, :],e=0.1) # 행동 결정\n","    new, reward, done, _ = env.step(action) # 행동에 따른 보상과 새로운 상태\n","    Q[state,action] = reward + discount*np.max(Q[new, :]) # Q함수 업데이트\n","    state = new  # 상태 업데이트"],"metadata":{"id":"_P0zZPDo8GDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Q[1:-1,[0,2]]  # 최종 Q함수"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"opYCQ8ob_lMW","outputId":"d9eae4d6-f639-4d78-8b85-64438b5a2b3b","executionInfo":{"status":"ok","timestamp":1756037817185,"user_tz":-540,"elapsed":4,"user":{"displayName":"colab c","userId":"17273071048194274415"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.   , 0.81 ],\n","       [0.729, 0.9  ],\n","       [0.81 , 1.   ]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# 연습문제"],"metadata":{"id":"wGngez-aMPZZ"}},{"cell_type":"markdown","source":["1. 위의 예제에서 다음을 변경하며 수행하시오.<br>\n","(1) policy 함수에서 e를 0.2으로 변경하였을 때, 최종적인 Q함수는 어떻게 주어지는가? [sH,s0,s1,s2,sD]와 같이 상태를 표시할 때, Q(s2,L)의 값은 얼마인가? 왜 그런지 설명하시오.<br>\n","(2) discount가 0.7일 때, policy 함수에서 e를 0.1로 두고 충분히 학습시키면 최종적인 Q함수는 어떻게 주어지는가?"],"metadata":{"id":"WB7Otjj1NUrd"}},{"cell_type":"code","source":[],"metadata":{"id":"lARei75UC2pg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. 위의 예제에서 아래와 같이 환경을 생성하고 Q학습을 수행하시오. 이 경우는 2차원 공간이기 때문에 4개의 행동이 모두 허용되어야 한다. policy 함수 및 다른 코드를 적절히 변경해가며 수행하시오. 최종적인 Q함수가 적절한지 확인하시오."],"metadata":{"id":"WfQz62pJOTH-"}},{"cell_type":"code","source":["env = gym.make('FrozenLake-v1',desc=[\"SFFF\",\"FHFH\",\"FFFH\",\"HFFG\"],is_slippery=False)"],"metadata":{"id":"MISEe6xNR-nd"},"execution_count":null,"outputs":[]}]}