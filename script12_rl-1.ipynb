{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo8WJWYS3jPa"
   },
   "source": [
    "<h1>강화학습</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9289,
     "status": "ok",
     "timestamp": 1756037807090,
     "user": {
      "displayName": "colab c",
      "userId": "17273071048194274415"
     },
     "user_tz": -540
    },
    "id": "Z6fTCJpy3aHY",
    "outputId": "8ef6cd5f-4ff2-40da-ecd6-c241f53c611a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.23 in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from gym==0.23) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym==0.23) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym==0.23) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# gym 라이브러리 설치\n",
    "# 강화학습 연구와 교육을 위해 각종 환경과 API를 제공\n",
    "!pip install gym==0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1756037811936,
     "user": {
      "displayName": "colab c",
      "userId": "17273071048194274415"
     },
     "user_tz": -540
    },
    "id": "0VNyrAFV4jGw",
    "outputId": "8f4dcacf-a07e-460a-a976-8dd37621a26a"
   },
   "outputs": [],
   "source": [
    "# 라이브러리\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 경고 메시지 출력하지 않기\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzbeDpgk456t"
   },
   "source": [
    "# Fronzen Lake + Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAx0F6zL8q8o"
   },
   "source": [
    "환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gTNJu1k945nb"
   },
   "outputs": [],
   "source": [
    "# 강의에서 다룬 Fronzen Lake 예제\n",
    "# 상태: 구멍(H)-출발(S)-얼음(F)-얼음(F)-목적지(G) 의 구조\n",
    "# 행동: 0(LEFT), 1(DOWN), 2(RIGHT), 3(UP)\n",
    "#       여기서는 2개만 허용: 0(LEFT), 2(RIGHT)\n",
    "# 보상: G은 +1, 나머지는 0\n",
    "env = gym.make('FrozenLake-v1',desc=['HSFFG'],is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mW25lsKQ8smA"
   },
   "source": [
    "가치함수와 정책함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1756037815217,
     "user": {
      "displayName": "colab c",
      "userId": "17273071048194274415"
     },
     "user_tz": -540
    },
    "id": "YPk6ET-t5Ibr",
    "outputId": "bad6f92e-291c-445d-c48c-c9aec229364b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가치함수: Q테이블, 0으로 초기화\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "20IMRyJS7hyo"
   },
   "outputs": [],
   "source": [
    "# 정책 함수, e는 e-greedy 변수\n",
    "# 4의 행동에 대한 Q값(L,D,R,U)을 받아 행동을 결정\n",
    "# 이 중에서 L/R만 사용\n",
    "def policy(qs,e=0):\n",
    "  if e > random.random():\n",
    "    return random.choice([0,2])\n",
    "  else:\n",
    "    if qs[0] > qs[2]: return 0\n",
    "    elif qs[0] < qs[2]: return 2\n",
    "    else: return random.choice([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxukz6vG80xz"
   },
   "source": [
    "모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_P0zZPDo8GDk"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m   action = policy(\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m,e=\u001b[32m0.1\u001b[39m) \u001b[38;5;66;03m# 행동 결정\u001b[39;00m\n\u001b[32m      9\u001b[39m   new, reward, done, _ = env.step(action) \u001b[38;5;66;03m# 행동에 따른 보상과 새로운 상태\u001b[39;00m\n\u001b[32m     10\u001b[39m   Q[state,action] = reward + discount*np.max(Q[new, :]) \u001b[38;5;66;03m# Q함수 업데이트\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "num_episodes = 100   # 필요하다면 더 늘려서 수행\n",
    "discount = 0.9\n",
    "Q = Q*0.0  # Q함수 초기화\n",
    "for i in range(num_episodes):\n",
    "  state = env.reset()   # 초기 상태로 리셋\n",
    "  done = False\n",
    "  while not done:\n",
    "    action = policy(Q[state, :],e=0.1) # 행동 결정\n",
    "    new, reward, done, _ = env.step(action) # 행동에 따른 보상과 새로운 상태\n",
    "    Q[state,action] = reward + discount*np.max(Q[new, :]) # Q함수 업데이트\n",
    "    state = new  # 상태 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1756037817185,
     "user": {
      "displayName": "colab c",
      "userId": "17273071048194274415"
     },
     "user_tz": -540
    },
    "id": "opYCQ8ob_lMW",
    "outputId": "d9eae4d6-f639-4d78-8b85-64438b5a2b3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[1:-1,[0,2]]  # 최종 Q함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGngez-aMPZZ"
   },
   "source": [
    "# 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB7Otjj1NUrd"
   },
   "source": [
    "1. 위의 예제에서 다음을 변경하며 수행하시오.<br>\n",
    "(1) policy 함수에서 e를 0.2으로 변경하였을 때, 최종적인 Q함수는 어떻게 주어지는가? [sH,s0,s1,s2,sD]와 같이 상태를 표시할 때, Q(s2,L)의 값은 얼마인가? 왜 그런지 설명하시오.<br>\n",
    "(2) discount가 0.7일 때, policy 함수에서 e를 0.1로 두고 충분히 학습시키면 최종적인 Q함수는 어떻게 주어지는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lARei75UC2pg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfQz62pJOTH-"
   },
   "source": [
    "2. 위의 예제에서 아래와 같이 환경을 생성하고 Q학습을 수행하시오. 이 경우는 2차원 공간이기 때문에 4개의 행동이 모두 허용되어야 한다. policy 함수 및 다른 코드를 적절히 변경해가며 수행하시오. 최종적인 Q함수가 적절한지 확인하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MISEe6xNR-nd"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',desc=[\"SFFF\",\"FHFH\",\"FFFH\",\"HFFG\"],is_slippery=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
